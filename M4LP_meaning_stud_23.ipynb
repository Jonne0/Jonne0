{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2 ([M4LP](https://osiris.uu.nl/osiris_student_uuprd/OnderwijsCatalogusSelect.do?selectie=cursus&collegejaar=2022&cursus=KI3V21001))\n",
        "\n",
        "The assignment covers lexical semantics and reasoning.  \n",
        "<font color=\"red\">**The rules to follow**:</font>  \n",
        "* Don't delete any initially provided cells, either text or code cells (but you should delete certain lines in the cells, continue reading).\n",
        "* Don't delete the exercise code header `#...# EXERCISE n #..#` lines and the `# TEST` lines. \n",
        "* Don't change the names of provided functions and variables. \n",
        "* If you skip an exercise, then delete all lines in the cell following the header `#...# EXERCISE n #..#` but leave its backup part--the line with `IFSKIPPED` and its following lines.\n",
        "* If you solve an exercise, then delete its corresponding backup part starting with `IFSKIPPED` and the following lines. \n",
        "* Use global vars throughout your code and change only those globals vars that are explicitly instructed. \n",
        "* For `#TEST` cells, if its output is coming from your code, then leave it; otherwise clear the output of the cell as it is uninformative and clutters the ipynb. \n",
        "* For Text cells, you are expected to insert your input only in the cells that come with a red section title. \n",
        "* Name the ipynb file with your group number, e.g., `01.ipynb` or `11.ipynb`.\n",
        "\n",
        "<font color=\"red\">You following these rules helps us to grade the submissions relatively efficiently. If these rules are violated, a submission will be subject to penalty points.</font>  \n",
        "\n",
        "<font color=\"red\">**IMPORTANT**</font>: you are strongly encouraged to use Google Colab when solving the exercises. Setting the common environment prevents students and teachers from various headaches related to cross-platform variations, module/package versioning, and unpredicted behaviour of the code. In this way, we try that you spend as much time as possible on coding during the course rather than on installations. Moreover, colab notebooks are very practical for group collaboration as they come with version history and several persons can work on the same notebook (not simultaneously though).  \n",
        "You are still free to solve the exercises on your own machine but in the end, make sure that your solutions also work in the colab environment. \n",
        "\n",
        "by L.abzianidze@uu.nl"
      ],
      "metadata": {
        "id": "7gbltP-WdxjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "~~Delete this text and write instead of it your:~~\n",
        "* ~~group number (same as the file name, for sanity chack)~~\n",
        "* ~~a list of group members names (NOT student IDs)~~\n",
        "* ~~who contributed to which exercises (you don't need to be very detailed)~~ "
      ],
      "metadata": {
        "id": "nNT1WNEnlkBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "_EU0aNkSBYGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "if spacy.__version__ != '3.5.2':\n",
        "    print(f\"spaCy v={spacy.__version__} but it should be 3.5.2\\nForce install 3.5.2 with the next cell\")"
      ],
      "metadata": {
        "id": "Ev0xSxOdzBTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# may require environment restart\n",
        "# !pip install spacy==3.5.2"
      ],
      "metadata": {
        "id": "SNxchUuszIOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3blhWf07xC1n"
      },
      "outputs": [],
      "source": [
        "# may require environment restart\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assigntools package is a course specific collection of useful tools \n",
        "!rm -fr assigntools # helps to rerun this cell witthout errors, if recloning needed \n",
        "! git clone https://github.com/kovvalsky/assigntools.git"
      ],
      "metadata": {
        "id": "kPIBvjxg9aN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import sys\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import spacy\n",
        "# Course-specific package\n",
        "from assigntools.M4LP.A1 import read_pickle, write_pickle\n",
        "from assigntools.M4LP.A2 import evaluate_contextual_lex_rel, taged2offsets, show_tableau, LangPro"
      ],
      "metadata": {
        "id": "m267kLd0Bv1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "print(f\"spaCy version: {spacy.__version__}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"NLTK version: {nltk.__version__}\")"
      ],
      "metadata": {
        "id": "P3JBRKtzxSRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de0627b-16cc-4b5f-d9dc-aa0d4ed001b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy version: 3.5.2\n",
            "Python version: 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "NLTK version: 3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User modules\n",
        "\n",
        "Import all modules here what you might need in addition to what is already imported."
      ],
      "metadata": {
        "id": "HcD8C5xA1isO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT ALL ADDED AND NECESSARY MODULES HERE (IF ANY)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pTz2S1M61vAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet"
      ],
      "metadata": {
        "id": "WvRSNrv9FTPY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkIOb4z--VDV"
      },
      "source": [
        "The code shows how to get the noun(!) sysnsets for `lecture` and for each sysnset to show its definition and examples and to list its word senses. This code attempts to show how to get all the info what is displayed in the [online results](http://wordnetweb.princeton.edu/perl/webwn?s=lecture&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=1&o5=&o9=&o6=&o3=&o4=&h=00000).\n",
        "More details about `wn.Synset` and `wn.Lemma` classes can be found [here](https://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html) and [here](https://www.nltk.org/howto/wordnet.html).  \n",
        "<font color=\"red\">It is important that you understand this code well as it will help you with other WordNet exercises</font>.  \n",
        "Note that NLTK uses WordNet 3.0 while the online browser 3.1. So, if there are some mismatches between these two, one of the reasons can be different versions.  \n",
        "If you prefer more graphical visualization of wordnet, check [visuwords](https://visuwords.com/). Warning: it might get pretty messy when large synsets are explored.  \n",
        "Definitions of certain technical terms of WordNet can be looked up [here](https://wordnet.princeton.edu/documentation/wngloss7wn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_mqDU35DUMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b534ed4-02bd-424f-cc95-332f28618050"
      },
      "source": [
        "for synset in wn.synsets('lecture', pos=wn.NOUN):\n",
        "    all_examples = ','.join([ f'\"{e}\"' for e in synset.examples() ])\n",
        "    print(f'{synset}:\\n\\t({synset.definition()})\\n\\t{all_examples}')\n",
        "    for l in synset.lemmas():\n",
        "        # find a sense number of the lemma of the target synset.\n",
        "        # For this, first, we find all synsets for this lemma and \n",
        "        # in this synset list we find the position+1 of the target synset \n",
        "        all_synsets_of_l = wn.synsets(l.name())\n",
        "        sense_num = all_synsets_of_l.index(l.synset()) + 1\n",
        "        print(f\"\\t{l.name()}#{sense_num}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('lecture.n.01'):\n",
            "\t(a speech that is open to the public)\n",
            "\t\"he attended a lecture on telecommunications\"\n",
            "\tlecture#1\n",
            "\tpublic_lecture#1\n",
            "\ttalk#4\n",
            "Synset('lecture.n.02'):\n",
            "\t(a lengthy rebuke)\n",
            "\t\"a good lecture was my father's idea of discipline\",\"the teacher gave him a talking to\"\n",
            "\tlecture#2\n",
            "\tspeech#6\n",
            "\ttalking_to#1\n",
            "Synset('lecture.n.03'):\n",
            "\t(teaching by giving a discourse on some subject (typically to a class))\n",
            "\t\n",
            "\tlecture#3\n",
            "\tlecturing#1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contextual lexical relation\n",
        "\n",
        "We will be working with the `context_ppdb` dataset from [Shwartz et al (2016)](https://aclanthology.org/S16-2013/). The examples from the dataset can be found in Table 1 of the paper and in the dataset itself. It is also recommended to read `README.txt` found in the dataset archive. We will use `dataset.tsv` file for the exercise (as we are not doing any model training).\n",
        "\n",
        "In a nutshell, the task is to guess a lexical relation that holds between the senses of `x` and `y` that they have in their corresponding contexts. We will divide the task into two tasks. First will be the word sense disambiguation (WSD) for `x/y` in their contexts, and the second will be to predict a lexical relation between the word senses based on WordNet."
      ],
      "metadata": {
        "id": "l_e10o69uisu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the context_ppdb dataset\n",
        "!wget https://naturallogic.pro/_files_/download/context_ppdb_fine_human_precise.zip\n",
        "!unzip -o context_ppdb_fine_human_precise.zip -d context_ppdb"
      ],
      "metadata": {
        "id": "3WVcIUZfJsRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# viewing top 10 lines of the dataset\n",
        "!head context_ppdb/dataset.tsv"
      ],
      "metadata": {
        "id": "zXsMRUMuSnGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99a20f0-c232-42d9-aeb6-fa663a01f8e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bicycle\triding\tBolotta recounted finding 22 sharpened <x>bicycle</x> spokes jabbed into the lawn while she was out with the lawn mower.\tA lesser known work of Hopper's, 'Bridal Path' shows a horseback <y>riding</y> path in Central Park.\tother-related\t0.6\n",
            "photo\tpicture\tSome of the mission will include examining the surface for sources of water, and taking comparison <x>photos</x> of the light side and dark sides of the surface.\tThe trial judge, His Honour Judge Michael Murphy QC, who had previously ordered the jury not to consult the Internet, did not halt the prosecution as he felt 'satisfied' the jury hadn't seen the <y>picture</y>.\tequivalence\t1.0\n",
            "catch\tfish\tDepartment officer Chris Mitchell says the fishermen had traversed 200 miles of Commonwealth waters and were inside the three nautical mile state limit when they were <x>caught</x>.\tA 375 million-year old fossilised umbilical cord indicates that placoderm, thought to be ancestors of modern <y>fish</y>, are actually closer to sharks and rays.\tindependent\t1.0\n",
            "fire\tpolice\tThe submarine had 208 people aboard—three times the normal crew—when the <x>fire</x> fighting system was triggered by an unknown cause, flooding a forward compartment with Freon gas which is used to extinguish fires by removing oxygen from the atmosphere.\tThe <y>police</y> have said that there are 100 dead in Chennai city alone.\tindependent\t1.0\n",
            "boy\tchild\tHe married a woman he meet at college, Norma Kaphan and they had two <x>boys</x>, Peter and Christopher.\tThe court found that he, along with his son Franck V. and Franck's former spouse, Patricia M., was one the instigators of a sex ring that abused 45 <y>children</y>, mostly in the couple's flat.\tforward_entailment\t1.0\n",
            "fun\tswing\tHowever, mathematics enthusiasts often seek them for <x>fun</x>, and the GIMPS community is already looking forward to finding a 100 million digit prime that will qualify for a $150,000 prize from the EFF. Great Internet Mersenne Prime Search\tThe ride is called a 'LollySwing', which is located in Kiddyland, where the riders sit in <y>swings</y> while the machine spins them around.\tindependent\t0.6\n",
            "car\twindows\tMogale's crimes lasted a year from March 2008 to March 2009, coming to an end when police were passed his <x>car</x>'s number plate information following a teen's disappearance.\tFour men attacked patrons of a Pizzeria, knocking a woman unconscious on the footpath and smashing <y>windows</y>.\tindependent\t1.0\n",
            "talk\tphone\tGoogle, Yahoo, and Canonical also gave <x>talks</x> about Google Gears, Yahoo UI, and Ubuntu Mobile.\tThe post of controller became vacant last year when Lesley Douglas resigned following the scandal over crude <y>phone</y> calls made by Russell Brand and Jonathan Ross to actor Andrew Sachs during a pre-recorded music programme.\tindependent\t0.6\n",
            "boat\tvehicle\tThe river is closed to <x>boats</x> as rescue operations continue, and a 22-strong team has been dispatched from the national police, comprising six forensics experts, five disaster victim identification specialists, and eleven investigators.\tAccording to BBC Cambridgeshire presenter Chris Mann, the car 'suddenly accelerated' into the rear of the <y>vehicle</y>.\tforward_entailment\t0.8\n",
            "market\toil\tAnalysts noted that this move has the potential to financially hurt Google, which has a somewhat limited share of the Internet search <x>market</x> in China, which is dominated by the Chinese-based Google-like website Baidu.\tThe United States government has filed a civil lawsuit against BP Exploration (Alaska) Inc. (BPXA) alleging that the company 'violated federal clean air and water laws' by 'illegally discharging' more than 200,000 gallons crude <y>oil</y> during two <y>oil</y> spills in 2006 on Alaska's North Slope in Prudhoe Bay.\tindependent\t0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex1[?pt]: Reading data\n",
        "\n",
        "Write `read_data` function to read data from the file. Before reading the data from the file, understand the content & format of the file. \n",
        "It is not a good practice when the original texts are changed, like in this dataset, where tags `<x>` and `<y>` are used to mark occurences of target words. Because of this, we first need to clean the texts from the tags but save the character offsets of the tagged tokens (to avoid information loss). The cleaned sentences will later be used as an input to spaCy.  \n",
        "To help you with replacing tagged word info with character offsets, we provide you a ready function [tagged2offsets](https://github.com/kovvalsky/assigntools/blob/main/M4LP/A2.py) that does this (it was imported in the beginning). "
      ],
      "metadata": {
        "id": "BjhmgMAMT4Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST: a real example from the dataset\n",
        "s = \"Studies have shown that <x>drinks</x>, especially sweetened, contribute to obesity among adults and children, leading to diseases like diabetes. 'Eight out of ten <x>drinks</x> sold in California public schools are sports <x>drinks</x>,' Padilla said, citing information from the California Department of Public Health.\"\n",
        "cleaned_s, offsets = taged2offsets(\"x\", s)\n",
        "print(cleaned_s)\n",
        "for start, end in offsets:\n",
        "    print(f\"{cleaned_s[start:end]} at {start}:{end}\")"
      ],
      "metadata": {
        "id": "98-wRUYVArlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Studies have shown that drinks, especially sweetened, contribute to obesity among adults and children, leading to diseases like diabetes. 'Eight out of ten drinks sold in California public schools are sports drinks,' Padilla said, citing information from the California Department of Public Health.\n",
        "drinks at 24:30\n",
        "drinks at 156:162\n",
        "drinks at 208:214\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qUbvTQgw3ypL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 1 ##################################\n",
        "################################################################################\n",
        "\n",
        "def read_data(file_path):\n",
        "    \"\"\" Read the samples from the data file.\n",
        "        Return a list of samples, where each list element is a dictionary\n",
        "        {'x':(x, tag_free_context_x, offsets_x), 'y':(y, tag_free_context_y, offsets_y), \n",
        "         'r':semantic_relation, 'c':confidence }.\n",
        "        x and y are target words for left and right contexts while \n",
        "        tag_free_context_* are corresponding context sentences without <x/y> tags.\n",
        "        offsets_x/y give positions of tagged words in the tag_free_context_x/y  \n",
        "    \"\"\"\n",
        "    # use provided taged2offsets function to get offsets of tagged words\n"
      ],
      "metadata": {
        "id": "S0miS-XKTnDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_data('context_ppdb/dataset.tsv')\n",
        "\n",
        "#IFSKIPPED\n",
        "# data = read_pickle('data.pkl') "
      ],
      "metadata": {
        "id": "cnlmq-gt-bPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST EX1: compare(!) this output to the corresponding lines in the data file\n",
        "# in order to better understand what kind of (weird) samples the data contains\n",
        "print(data[0])\n",
        "print(data[667])\n",
        "print(data[766])\n",
        "print(data[3338])\n",
        "print(f\"The data size = {len(data)}\")"
      ],
      "metadata": {
        "id": "6VPvhOMDNlck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{'x': ('bicycle', 'Bolotta recounted finding 22 sharpened bicycle spokes jabbed into the lawn while she was out with the lawn mower.', [(39, 46)]), 'y': ('riding', \"A lesser known work of Hopper's, 'Bridal Path' shows a horseback riding path in Central Park.\", [(65, 71)]), 'r': 'other-related', 'c': '0.6'}\n",
        "{'x': ('girl', 'One girl is still missing.', [(4, 8)]), 'y': ('woman', \"Activists have sought women's right to vote in Saudi Arabia for years.\", [(22, 27)]), 'r': 'other-related', 'c': '0.6'}\n",
        "{'x': ('family', 'Father Yus Mawengkang, a Catholic priest in Papua, said the family had been transported to the island on fishing boats and hoped that they would be collected by Australian authorities.', [(60, 66)]), 'y': ('child', 'He is survived by three children, along with grandchildren and great-grandchildren.', [(24, 32), (50, 58), (74, 82)]), 'r': 'other-related', 'c': '0.8'}\n",
        "{'x': ('goal', 'Scientific goals.', [(11, 16)]), 'y': ('soccer', \"Fans of the Netherlands' soccer team were wildly celebrating too.\", [(25, 31)]), 'r': 'independent', 'c': '0.8'}\n",
        "The data size = 3404\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hG-JLMoL342j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST: we also provide you with a function that evaluates predicted relations\n",
        "# wrt the gold relations in data. Let's see if one always predicts 'independent'\n",
        "# relation, what will be its accuracy and confusion matrix wrt the gold relations\n",
        "evaluate_contextual_lex_rel(['independent']*len(data), data, draw=True)\n",
        "# Note that 'independent' is the majority class baseline "
      ],
      "metadata": {
        "id": "x2lDPm9vBqCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex2[?pt]: Processing with spaCy\n",
        "\n",
        "In order to guess the sense of a word in a context, it is helpful to know its POS tag. To find out the POS tags, we process context sentences with spaCy. To make processing faster, you can use `.pipe()` method (see Assignmnet 1) to parse all sentences in a single run.  \n",
        "Note that we use `en_core_web_md` as it is the smallest model that comes with word vectors (and we will use them in latter exercises). "
      ],
      "metadata": {
        "id": "vrWnexJie9JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 2 ##################################\n",
        "################################################################################\n",
        "\n",
        "def spacy_process(data):\n",
        "    \"\"\" Takes the data variable as an input and processes the sentences with spacy.\n",
        "        Returns a list of samples, where each sample is a dictionary\n",
        "        {'x': (pos_x, doc_x), 'y': (pos_y, doc_y)}, where pos and doc are\n",
        "        spaCy's pos tag (token.pos_) for a corresponging target word and \n",
        "        Doc object of the corresponding sentecne. \n",
        "        Sometimes target words have several occurences in a sentence \n",
        "        that might get different pos tags. In this case, for the sake of determinism,\n",
        "        pick the first from the alphabetical order, e.g., ADJ is prefered over NOUN.\n",
        "        The list order follows the order of samples in the data  \n",
        "    \"\"\"\n",
        "    nlp = spacy.load(\"en_core_web_md\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sGyP8uebe9TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "data[0]"
      ],
      "metadata": {
        "id": "nHOOzSdJTtl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{'x': ('bicycle',\n",
        "  'Bolotta recounted finding 22 sharpened bicycle spokes jabbed into the lawn while she was out with the lawn mower.',\n",
        "  [(39, 46)]),\n",
        " 'y': ('riding',\n",
        "  \"A lesser known work of Hopper's, 'Bridal Path' shows a horseback riding path in Central Park.\",\n",
        "  [(65, 71)]),\n",
        " 'r': 'other-related',\n",
        " 'c': '0.6'}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EF1WyvYg4GxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes ~40 sec with .pipe()\n",
        "docs_data = spacy_process(data)\n",
        "\n",
        "#IFSKIPPED\n",
        "# docs_data = read_pickle('docs_data.pkl')"
      ],
      "metadata": {
        "id": "pmyD4ZZeAsXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "print(data[766]['y'])\n",
        "print(docs_data[766]['y'])\n",
        "print(f\"{'-':-^20}\")\n",
        "print(data[3068]['x'])\n",
        "print(docs_data[3068]['x'])\n",
        "print(f\"{'-':-^20}\")\n",
        "print(data[3338]['x'])\n",
        "print(docs_data[3338]['x'])"
      ],
      "metadata": {
        "id": "3sCf1Q3tcpTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "('child', 'He is survived by three children, along with grandchildren and great-grandchildren.', [(24, 32), (50, 58), (74, 82)])\n",
        "('NOUN', He is survived by three children, along with grandchildren and great-grandchildren.)\n",
        "--------------------\n",
        "('front', \"Several officers in white forensic suits were examining the barricaded area and a huge white tent was erected in front of the house's front door.\", [(113, 118), (134, 139)])\n",
        "('ADJ', Several officers in white forensic suits were examining the barricaded area and a huge white tent was erected in front of the house's front door.)\n",
        "--------------------\n",
        "('goal', 'Scientific goals.', [(11, 16)])\n",
        "('NOUN', Scientific goals.)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "96Uzx7s74L_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST EX2 \n",
        "#checking what is the counts of pos tags assigned by spaCy en_core_web_md(!) to the target words \n",
        "word_pos_cnt = Counter(s[i][0] for s in docs_data for i in 'xy' )\n",
        "print(word_pos_cnt)\n",
        "print(f\"{sum(word_pos_cnt.values())} pos tagged words show that we have tags for all target words\")"
      ],
      "metadata": {
        "id": "4fgzlrD_krRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Counter({'NOUN': 4921, 'VERB': 946, 'PROPN': 765, 'ADJ': 174, 'INTJ': 2})\n",
        "6808 pos tagged words show that we have tags for all target words\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vrSiND9g4gb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WSD\n",
        "\n",
        "After we have processed context sentences, we have POS tags for the target words. This will help us to narrow down the search space of word senses. Now it is time to do Word Sense Disambiguation (WSD) for each word in its context sentence. You are supposed to write three functions that predict WordNet sense for each word in the context for the entire data. Each of these functions take `data` and `docs_data` and returns a list of dictionaries with synsets of `x` and `y` words. The functions will differ the way they predict senses. \n",
        "\n",
        "\n",
        "* `most_frequent_sense()` - picks the first sense of the word, which is also the most frequent sense of the word.pos (i.e., word.pos.01).\n",
        "* `simple_lesk_sense()` - picks the sense whose description and/or examples has largest overlap with the context sentence.\n",
        "* `vector_lesk_sesne()` - picks the sense whose description and/or examples vector is most similar to the vector of the context sentence.\n",
        "\n",
        "<font color=\"red\">IMPORTANT</font>: WordNet covers the words belonging to four classes (noun, verb, adjective, and adverb). In case spaCy assigned a POS tag that is none of these four, the wsd functions should be able to have some fallback pos tag in such cases to avoid runtime errors. Noun is a good option for such a fallback pos tag. For example, if a word gets proper name POS tag, a wsd can assume that its tag is Noun.\n",
        "\n",
        "Note that whether the WordNet description and examples are used alone or together, it is up to you. If you want to have a relativelyt high-performing system in the end, you might want to explore different settings.  \n",
        "\n",
        "Potentially useful links: [link1](https://github.com/Akirato/Lesk-Algorithm/blob/master/leskAlgorithm.py), [link2](https://medium.com/analytics-vidhya/comparative-word-sense-disambiguation-1c3f0f4be1fa), [link3](https://www.nltk.org/howto/wsd.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "vu2mvasrXWdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex3[?pt]: Most frequent sense"
      ],
      "metadata": {
        "id": "SPRGs97Fjdxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 3 ##################################\n",
        "################################################################################\n",
        "\n",
        "def most_frequent_sense(data, docs_data):\n",
        "    \"\"\" Takes data and docs_data, and return a list of dictionaries\n",
        "        {'x': synset_of_x, 'y': synset_of_y } where synset_of_x/y is \n",
        "        the first sense of x/y in the context_x/y.\n",
        "        Note that pos tag of x/y is retrieved from docs_data and used\n",
        "        to restric the possible senses for a word.\n",
        "        If synset_of_x/y cannot be found (e.g., x/y is an unknown word), \n",
        "        return None for the corresponding x/y key.\n",
        "        The order in the returned list follows the orders in data and docs_data.\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "rJeiZ0kpXUtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MFS_data = most_frequent_sense(data, docs_data)"
      ],
      "metadata": {
        "id": "AD5lFhcd6AfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST EX3: note that in this output, \"sit\" in 628_x is identifiet as Noun but\n",
        "# WordNet doesn't have noun sense of \"sit\", hence the synset is None\n",
        "for i in [628, 766, 3338]:\n",
        "    for xy in 'xy':\n",
        "        print(data[i][xy][:2])\n",
        "        print(docs_data[i][xy])\n",
        "        print(MFS_data[i][xy])\n",
        "    print(f\"{'':-^20}\")"
      ],
      "metadata": {
        "id": "f0QJYPd0oDCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "('sit', 'In West Bengal, the ruling Left Front organised sit-ins before the offices of oil PSUs.')\n",
        "('NOUN', In West Bengal, the ruling Left Front organised sit-ins before the offices of oil PSUs.)\n",
        "None\n",
        "('stand', 'It was worth my while to try to stand up and punch with him.')\n",
        "('VERB', It was worth my while to try to stand up and punch with him.)\n",
        "Synset('stand.v.01')\n",
        "--------------------\n",
        "('family', 'Father Yus Mawengkang, a Catholic priest in Papua, said the family had been transported to the island on fishing boats and hoped that they would be collected by Australian authorities.')\n",
        "('NOUN', Father Yus Mawengkang, a Catholic priest in Papua, said the family had been transported to the island on fishing boats and hoped that they would be collected by Australian authorities.)\n",
        "Synset('family.n.01')\n",
        "('child', 'He is survived by three children, along with grandchildren and great-grandchildren.')\n",
        "('NOUN', He is survived by three children, along with grandchildren and great-grandchildren.)\n",
        "Synset('child.n.01')\n",
        "--------------------\n",
        "('goal', 'Scientific goals.')\n",
        "('NOUN', Scientific goals.)\n",
        "Synset('goal.n.01')\n",
        "('soccer', \"Fans of the Netherlands' soccer team were wildly celebrating too.\")\n",
        "('NOUN', Fans of the Netherlands' soccer team were wildly celebrating too.)\n",
        "Synset('soccer.n.01')\n",
        "--------------------\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6xsSlXzvRmG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex4[?pt]: Simple Lesk\n",
        "\n",
        "Simplest is to use [NLTK's Lesk](https://www.nltk.org/howto/wsd.html). You can read more about Lesk Algorithm [here](https://en.wikipedia.org/wiki/Lesk_algorithm). You are also welcome to write your own Lesk version as NLTK's one is not performing well."
      ],
      "metadata": {
        "id": "s6vkW4WejT51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 4 ##################################\n",
        "################################################################################\n",
        "\n",
        "def simple_lesk_sense(data, docs_data):\n",
        "    \"\"\" Takes data and docs_data, and return a list of dictionaries\n",
        "        {'x': synset_of_x, 'y': synset_of_y } where synset_of_x/y is \n",
        "        the sense of x/y in the context_x/y detecting by Lesk-like algorithm,\n",
        "        which selects a sense based on the size of the overlap between the\n",
        "        context and the definition and/or examples of the possible synsets.\n",
        "        Note that pos tag of x/y is retrieved from docs_data and used\n",
        "        to restric the possible senses for a word.\n",
        "        The order in the returned list follows the orders in data and docs_data.\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "qB_CjuwmeTOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SIMPLE_LESK_data = simple_lesk_sense(data, docs_data)"
      ],
      "metadata": {
        "id": "hPQ__OQ7Fg1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST EX4\n",
        "# These are the same data samples as the ones above.\n",
        "# compare sense predictions from this to the previous function(s) and find the difference\n",
        "# IMPORTANT: depending on certain choice points, you might not get exactly the same\n",
        "# output as below and that's fine. \n",
        "for i in [628, 766, 3338]:\n",
        "    for xy in 'xy':\n",
        "        print(data[i][xy][:2])\n",
        "        print(docs_data[i][xy])\n",
        "        print(SIMPLE_LESK_data[i][xy])\n",
        "    print(f\"{'':-^20}\")"
      ],
      "metadata": {
        "id": "-lf-Rz06H0iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "('sit', 'In West Bengal, the ruling Left Front organised sit-ins before the offices of oil PSUs.')\n",
        "('NOUN', In West Bengal, the ruling Left Front organised sit-ins before the offices of oil PSUs.)\n",
        "None\n",
        "('stand', 'It was worth my while to try to stand up and punch with him.')\n",
        "('VERB', It was worth my while to try to stand up and punch with him.)\n",
        "Synset('digest.v.03')\n",
        "--------------------\n",
        "('family', 'Father Yus Mawengkang, a Catholic priest in Papua, said the family had been transported to the island on fishing boats and hoped that they would be collected by Australian authorities.')\n",
        "('NOUN', Father Yus Mawengkang, a Catholic priest in Papua, said the family had been transported to the island on fishing boats and hoped that they would be collected by Australian authorities.)\n",
        "Synset('syndicate.n.01')\n",
        "('child', 'He is survived by three children, along with grandchildren and great-grandchildren.')\n",
        "('NOUN', He is survived by three children, along with grandchildren and great-grandchildren.)\n",
        "Synset('child.n.04')\n",
        "--------------------\n",
        "('goal', 'Scientific goals.')\n",
        "('NOUN', Scientific goals.)\n",
        "Synset('goal.n.04')\n",
        "('soccer', \"Fans of the Netherlands' soccer team were wildly celebrating too.\")\n",
        "('NOUN', Fans of the Netherlands' soccer team were wildly celebrating too.)\n",
        "Synset('soccer.n.01')\n",
        "--------------------\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sALaMxeJTGZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex5[?pt]: Vector-based Lesk\n",
        "\n",
        "The simple Lesk algorith uses the size of the word overlap between the context and the synset gloss to predict the synset. Here we ask you to use vector (cosine) similarity instead of the size of overlap. In sapCy is very simple to check vector similarity between two spaCy's tokens/spans/docs. But this also means that we need to process glosses of relevant synsets with spaCy.  \n",
        "To make things relatively efficient, you can first extract all possibel glosses of target words of the data and process them with a singel `.pipe()` run of spaCy. It is up to you what will be gloss of teh synset, only examples, definitions, or both. Note that you can give several sentences as a singel string for processing to spaCy.  \n",
        "\n",
        "After processing all relevant glosses, one needs to keep processed glosses related to their corresponding synsets. With the help of this, then you can easily retrieve processed glosses when checking similarity to the context of a target word. \n",
        "\n",
        "Read about Vector similarity in spaCy in [sec. 8](https://course.spacy.io/en/chapter2). More about the spaCy's vectors can be found [here](https://spacy.io/api/vectors)."
      ],
      "metadata": {
        "id": "Axwpj85HjpLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 5 ##################################\n",
        "################################################################################\n",
        "\n",
        "def vector_lesk_sense(data, docs_data):\n",
        "    \"\"\" Takes data and docs_data, and return a list of dictionaries\n",
        "        {'x': synset_of_x, 'y': synset_of_y } where synset_of_x/y is \n",
        "        the sense of x/y in the context_x/y detecting by Lesk-like algorithm,\n",
        "        which selects a sense based on the size of the overlap between the\n",
        "        context and the definition and/or examples of the possible synsets.\n",
        "        Note that pos tag of x/y is retrieved from docs_data and used\n",
        "        to restric the possible senses for a word.\n",
        "        The order in the returned list follows the orders in data and docs_data.\n",
        "    \"\"\"\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_md\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ghPYMwpBeTVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if efficently implemented, takes <20sec\n",
        "VEC_LESK_data = vector_lesk_sense(data, docs_data)"
      ],
      "metadata": {
        "id": "81sS2U_DW23O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST EX5 \n",
        "# These are the same data samples as the ones above.\n",
        "# compare sense predictions from this to the previous function(s) and find the difference\n",
        "# IMPORTANT: depending on certain choice points, you might not get exactly the same\n",
        "# output as below and that's fine. \n",
        "for i in [628, 766, 3338]:\n",
        "    for xy in 'xy':\n",
        "        print(data[i][xy][:2])\n",
        "        print(docs_data[i][xy])\n",
        "        print(VEC_LESK_data[i][xy])\n",
        "    print(f\"{'':-^20}\")"
      ],
      "metadata": {
        "id": "8QXNvwu3kR3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a070b67a-190f-45a1-9633-1ac76386f254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('sit', 'In West Bengal, the ruling Left Front organised sit-ins before the offices of oil PSUs.')\n",
            "('NOUN', In West Bengal, the ruling Left Front organised sit-ins before the offices of oil PSUs.)\n",
            "None\n",
            "('stand', 'It was worth my while to try to stand up and punch with him.')\n",
            "('VERB', It was worth my while to try to stand up and punch with him.)\n",
            "Synset('digest.v.03')\n",
            "--------------------\n",
            "('family', 'Father Yus Mawengkang, a Catholic priest in Papua, said the family had been transported to the island on fishing boats and hoped that they would be collected by Australian authorities.')\n",
            "('NOUN', Father Yus Mawengkang, a Catholic priest in Papua, said the family had been transported to the island on fishing boats and hoped that they would be collected by Australian authorities.)\n",
            "Synset('family.n.08')\n",
            "('child', 'He is survived by three children, along with grandchildren and great-grandchildren.')\n",
            "('NOUN', He is survived by three children, along with grandchildren and great-grandchildren.)\n",
            "Synset('child.n.03')\n",
            "--------------------\n",
            "('goal', 'Scientific goals.')\n",
            "('NOUN', Scientific goals.)\n",
            "Synset('goal.n.04')\n",
            "('soccer', \"Fans of the Netherlands' soccer team were wildly celebrating too.\")\n",
            "('NOUN', Fans of the Netherlands' soccer team were wildly celebrating too.)\n",
            "Synset('soccer.n.01')\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex6[?pt]: Mapping relations\n",
        "\n",
        "To relate the word sense disambiguation to the task of predicting contextual lexical relation, we need to have a function that predicts one of the six lexical relations between two synsets. Based on the locations of the synsets in the WordNet, give a reasonable lexical relation between the synsets. For example, `forward_entailment`, `reverse_entailment`, and `equivalence` can be defined in terms of WordNet's hyponym/hypernym relations (see the note about the hypernymy/hyponymy relations below). `alternation` can be defined in terms of [co-hyponymy](https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy#Co-hyponyms). `other-related` can be defined based on other WordNet relations. One can also try to use `similar_tos()` relation which stands for `similar to` for adjectives."
      ],
      "metadata": {
        "id": "mnshCkT4g8DA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For using the hypernymy/hyponymy relation from WordNet, **you need too use its transitive version**.  \n",
        "\n",
        "The hypernymy/hyponymy relation in WordNet doesn't come with transitivity closure. For example, `dog` has a sense `dog.n.01 (a member of the genus Canis ...)` and it is more specific than `animal.n.01 (a living organism ...)`, but this relation cannot be captured with `wn.synset('dog.n.01').hypernyms()` because it lists the immediate hypernyms: `canine.n.02'` and `domestic_animal.n.01`.  \n",
        "The real picture in WordNet is the following: `dog.n.01` < `domestic_animal.n.01` < `animal.n.01` OR `dog.n.01` < `canine.02` < `carnivore.n.01` < ... < `chordate.n.01` < `animal.n.01`, where x < y denotes that x is more specific than y, i.e., x is a hyponym of y, i.e., y is a hypernym of x.  \n",
        "We would like to capture the transitivity closure of the hypernymy relation. The NLTK's [howto](https://www.nltk.org/howto/wordnet.html) about WordNet will help you in this.  "
      ],
      "metadata": {
        "id": "NvP4JFOPVBQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides defining `lex_rel`, explain and motivate your WordNet-based definitions of lexical relations. Giving examples will help the explanation.\n",
        "\n",
        "<font color=\"red\">█████ MOTIVATION █████</font>\n",
        "\n",
        "TYPE YOUR MOTIVATION HERE (don't delete the heaader)"
      ],
      "metadata": {
        "id": "INx550royB7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "################################## EXERCISE 7 ##################################\n",
        "################################################################################\n",
        "\n",
        "def lex_rel(ss1, ss2):\n",
        "    \"\"\" Takes two synsets and based on wordnet it outputs one of the following six\n",
        "        relations: 'independent', 'equivalence', 'forward_entailment',\n",
        "                   'reverse_entailment', 'alternation', 'other-related'\n",
        "        if one of the sensets is None, it returns 'independent'.  \n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return 'independent' # fallback option"
      ],
      "metadata": {
        "id": "kcdO37t5g8LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all together\n",
        "\n",
        "Now we are putting together the various WSD functions and the definition of lexical relations between synsets based on WordNet."
      ],
      "metadata": {
        "id": "vEH9lRpIiJBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in-context semantic relation classifier\n",
        "def contextual_lex_rel(data, docs_data, WSD_OR_OUT):\n",
        "    \"\"\" In addition to the raw data and processed data, it takes \n",
        "        the WSD function (most_frequent_sense, simple_lesk_sense, vector_lesk_sense)\n",
        "        or its output and \n",
        "        Returns a list of predictions, i.e. one of teh six relations\n",
        "    \"\"\" \n",
        "    # this construction is also called ternary operator\n",
        "    # https://stackoverflow.com/questions/394809/does-python-have-a-ternary-conditional-operator\n",
        "    wsd_data = WSD_OR_OUT if isinstance(WSD_OR_OUT, list) else WSD_OR_OUT(data, docs_data)\n",
        "    # predciting relations for each data sample\n",
        "    predicted_rels = [ lex_rel(d['x'], d['y']) for d in wsd_data ]\n",
        "    return predicted_rels"
      ],
      "metadata": {
        "id": "Ger8gNEYhTIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST: the number of predictions should be the size of data\n",
        "mfs_pred = contextual_lex_rel(data, docs_data, MFS_data)\n",
        "print(f\"Num of predictions = {len(mfs_pred)}\")\n",
        "# the predictions should only include at most six possible relations\n",
        "print(f\"A set of predictions = {set(mfs_pred)}\")"
      ],
      "metadata": {
        "id": "QyrMxO4Z07qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Num of predictions = 3404\n",
        "A set of predictions = {'equivalence', 'alternation', 'reverse_entailment', 'other-related', 'independent', 'forward_entailment'}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BhaHlkU9WCO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best performance\n",
        "\n",
        "The code displays performance of all combinations of wsd components and the lexical relation definition. \n",
        "\n",
        "<font color=\"red\">The groups of the top three perfroming systems will get additional bonus points (taking into account their positions `{1:5pt, 2:3pt, 3:2pt}`).</font>  \n",
        "Note that most frequent sense (MFS) baseline is usually very good when it comes to word sense disambiguation. So, if your system is not better than the one based on MFS, it's ok. Remember that in general WSD is a hard task, especially when WSD is done wrt WordNet that has so many fine-grained synsets per word."
      ],
      "metadata": {
        "id": "6rGtXtUOkbKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST final system evaluation\n",
        "for wsd, name in zip([MFS_data, SIMPLE_LESK_data, VEC_LESK_data], \n",
        "               \"mfs simple_lesk vector_lesk\".split()):\n",
        "    pred = contextual_lex_rel(data, docs_data, wsd)\n",
        "    acc = evaluate_contextual_lex_rel(pred, data)\n",
        "    print(f\"{name}: {acc}\")"
      ],
      "metadata": {
        "id": "_ZX2H7BokiA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning Exercise TO BE ADDED\n",
        "\n",
        "We show how reasoning is done with the natural tableau theorem prover, how certain inference relations are (in)correctly predicted, and how manually proving the lexical knowledge can help to find proofs.  \n",
        "First we need to create LangPro object that knows locaton of LangPro's and SICK files. "
      ],
      "metadata": {
        "id": "cg_xoYGOUvpD"
      }
    }
  ]
}